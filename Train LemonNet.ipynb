{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Train LemonNet.ipynb","provenance":[{"file_id":"1AwX1ECEKWSKbHxA1XruQrIePp9U1FywH","timestamp":1614491017986}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"hWTgrFnWEdMl"},"source":["!pip install torch\n","!pip install classy_vision\n","!pip install tensorboard\n","%load_ext tensorboard\n","!pip install psutil\n","!pip install segmentation-models-pytorch==0.1.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNjtUJKdh45y","executionInfo":{"status":"ok","timestamp":1616241370158,"user_tz":-540,"elapsed":2847,"user":{"displayName":"Ian M","photoUrl":"","userId":"02211834680249837440"}},"outputId":"cdff4133-d73d-459d-cdc1-1b0aa132dd90"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H11sKWduG0w6"},"source":["from shutil import copyfile\n","copyfile('drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/data/archive.hdf5','archive.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGewSlmXEyCn"},"source":["import os\n","import re\n","import h5py\n","import numpy as np\n","import torch.utils.data\n","from torch.utils.data import random_split\n","from classy_vision.dataset import ClassyDataset\n","from classy_vision.tasks import ClassificationTask\n","import torchvision.transforms as transforms\n","from classy_vision.dataset.transforms import build_transforms, ApplyTransformToKey\n","from classy_vision.dataset.transforms import LightingTransform\n","from PIL import Image\n","from math import ceil\n","import psutil\n","cpu_count = psutil.cpu_count()\n","\n","import logging\n","import os\n","logger = logging.getLogger()\n","logger.setLevel(os.environ.get(\"LOGLEVEL\", \"INFO\"))\n","\n","\n","class HiroshimaLemon(torch.utils.data.Dataset):\n","    \"\"\"HiroshimaLemon dataset.\"\"\"\n","\n","    def __init__(self, data='archive.hdf5', split='train', in_memory=True):\n","        assert os.path.exists(data), \"Data path '{}' not found\".format(data)\n","        splits = [\"train\", \"test\"]\n","        assert split in splits, \"Split '{}' not supported\".format(split)\n","        logger.info(\"Constructing HiroshimaLemon {}...\".format(split))\n","        self._data, self._split = data, split\n","        self._size = 0\n","        assert in_memory, \"Only in_memory implemented\"\n","        self._in_memory = in_memory\n","        self._construct_imdb()\n","\n","    def _construct_imdb(self):\n","        \"\"\"Constructs the imdb.\"\"\"\n","\n","        with h5py.File(self._data,'r') as hf:\n","\n","            if self._in_memory:\n","                self._imdb = {}\n","            if self._split == 'train':\n","                self._imdb_lookup = {}\n","                i = 0\n","                train = hf['train']\n","                self._class_ids = sorted(f for f in train.keys() if re.match(r\"^[0-9]+$\", f))\n","                self._class_id_cont_id = {v: i for i, v in enumerate(self._class_ids)}\n","                for class_id in self._class_ids:\n","                    cont_id = self._class_id_cont_id[class_id]\n","                    if self._in_memory:\n","                        self._imdb[cont_id] = train[class_id][()]\n","                    size = train[class_id].shape[0]\n","                    self._imdb_lookup.update(dict(zip(list(range(i,i+size)),list(zip(list(range(0,size)),[cont_id]*size)))))\n","                    i += size\n","                self._size = i\n","                logger.info(\"Number of images: {}\".format(i))\n","                logger.info(\"Number of classes: {}\".format(len(self._class_ids)))\n","            else:\n","                if self._in_memory:\n","                    self._imdb = hf['test'][()]\n","                    size = self._imdb.shape[0]\n","                    logger.info(\"Number of images: {}\".format(size))\n","                    self._size = size\n","\n","    def __getitem__(self, index):\n","        assert index >= 0 and index < self._size, \\\n","            \"Provided index {} must be in range [0, {}).\".format(index, self._size)\n","        if self._split == 'train':\n","          i, cont_id = self._imdb_lookup[index]\n","          return {'input':Image.fromarray(self._imdb[cont_id][i,:,:,:], 'RGB'),'target':cont_id}\n","        else:\n","          return Image.fromarray(self._imdb[index,:,:,:], 'RGB')\n","\n","    def __len__(self):\n","        return self._size\n","\n","class HiroshimaLemonClassy(ClassyDataset):\n","    def __init__(self, dataset, batchsize_per_replica, shuffle, transform, num_samples):\n","        super().__init__(dataset, batchsize_per_replica, shuffle, transform, num_samples)\n","\n","dataset = HiroshimaLemon()\n","\n","# Transformation pipeline\n","image_transform_train = transforms.Compose([\n","    transforms.RandomRotation(180, resample=False, expand=False),\n","    transforms.RandomPerspective(distortion_scale=0.05, p=0.5, interpolation=2, fill=0),\n","    transforms.RandomResizedCrop(240, scale=(2.66-0.2,2.66+0.2), ratio=(0.9, 1.1), interpolation=2),\n","    transforms.ToTensor(),\n","    LightingTransform(alphastd=0.1),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","transform_train = ApplyTransformToKey(\n","    transform=image_transform_train,\n","    key=\"input\",\n",")\n","\n","image_transform_test = transforms.Compose([\n","    transforms.Resize(240),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","valid = False\n","\n","if valid:\n","\n","  transform_test = ApplyTransformToKey(\n","      transform=image_transform_test,\n","      key=\"input\",\n","  )\n","\n","  train_test_split = 0.1\n","  dataset_train, dataset_test = random_split(dataset, [len(dataset) - ceil(len(dataset) * train_test_split), ceil(len(dataset) * train_test_split)])\n","\n","  dataset_train_classy = HiroshimaLemonClassy(\n","      dataset=dataset_train,\n","      batchsize_per_replica=8, \n","      shuffle=True, \n","      transform=transform_train,\n","      num_samples=None\n","  )\n","\n","  dataset_test_classy = HiroshimaLemonClassy(\n","      dataset=dataset_test,\n","      batchsize_per_replica=8,\n","      shuffle=True, \n","      transform=transform_test, \n","      num_samples=None\n","  )\n","  dataset_test_classy.set_num_workers(cpu_count)\n","\n","else:\n","\n","  dataset_test = HiroshimaLemon(split='test')\n","\n","  dataset_train_classy = HiroshimaLemonClassy(\n","      dataset=dataset,\n","      batchsize_per_replica=8, \n","      shuffle=True, \n","      transform=transform_train,\n","      num_samples=None\n","  )\n","\n","dataset_train_classy.set_num_workers(cpu_count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8y_8wx63l-9A"},"source":["import matplotlib.pyplot as plt\n","from random import randint\n","\n","classes = set()\n","fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 5))\n","axs = axs.flatten()\n","while True:\n","  example = dataset[randint(0,len(dataset)-1)]\n","  target = example['target']\n","  if target in classes:\n","    pass\n","  else:\n","    classes.add(target)\n","    axs[target].imshow(example['input'])\n","    axs[target].title.set_text(target)\n","    axs[target].set_axis_off()\n","  if len(classes)==4:\n","    break\n","plt.tight_layout()\n","fig.suptitle('Examples of lemons in the dataset: 0-high, 3-low quality')\n","fig.subplots_adjust(top=0.92)\n","fig.savefig('example_lemons.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMnvsZeNSW8P"},"source":["%tensorboard --logdir ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ug57QsnKNw0s"},"source":["from classy_vision.tasks import ClassificationTask\n","from classy_vision.optim import SGD, Adam\n","from classy_vision.optim.param_scheduler import LinearParamScheduler\n","from classy_vision.trainer import LocalTrainer\n","from classy_vision.models import build_model\n","from classy_vision.losses import build_loss\n","from classy_vision.meters import AccuracyMeter\n","from classy_vision.hooks import ProgressBarHook, LossLrMeterLoggingHook, CheckpointHook, TensorboardPlotHook\n","from torch.utils.tensorboard import SummaryWriter\n","from datetime import datetime\n","from pathlib import Path\n","import segmentation_models_pytorch as smp\n","from collections import defaultdict\n","from datetime import datetime\n","\n","now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","\n","classification_model = build_model({\"name\": \"efficientnet\",\n","                     \"model_name\":\"B1\",\n","                     \"num_classes\":4,\n","                     \"bn_momentum\":0.010,\n","                     \"bn_epsilon\":1e-3,\n","                     \"width_divisor\":8,\n","                     \"min_width\":None,\n","                     \"drop_connect_rate\":0.2,\n","                     \"use_se\":False})\n","\n","\n","model_type = 'UnetPlusPlus'\n","#model_type = 'Linknet'\n","#model_type = 'FPN'\n","#model_type = 'PSPNet'\n","\n","extra = defaultdict(dict)\n","extra['UnetPlusPlus'] = dict(decoder_attention_type=None,\n","                             decoder_use_batchnorm=True,\n","                             decoder_channels=(128, 64, 32, 16),\n","                             encoder_depth=4)\n","\n","segmentation_model = getattr(smp, model_type)(\n","    encoder_name=\"efficientnet-b1\",\n","    encoder_weights=None,\n","    in_channels=3,\n","    classes=1,\n","    activation='sigmoid',\n","    **extra[model_type]\n",")\n","\n","\n","#filename = 'unetplusplus_efficientnet_b1_20210223111032.pt'\n","#filename = 'pspnet_efficientnet_b1_20210228042756_15.pt' # So bad!\n","filename = 'unetplusplus_efficientnet_b1_20210228032202_8.pt'\n","\n","copyfile(f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/{filename}',filename)\n","segmentation_model.load_state_dict(torch.load(filename))\n","\n","class Model(torch.nn.Module):\n","    def __init__(self, segmentation_model, classification_model):\n","        super(Model, self).__init__()\n","        self.segmentation_model = segmentation_model\n","        for param in self.segmentation_model.parameters():\n","            param.requires_grad = False\n","        self.segmentation_model.eval()\n","        self.classification_model = classification_model\n","        self.training = True\n","\n","    def forward(self, x):\n","        mask = self.segmentation_model(x)\n","        return self.classification_model(mask*x)\n","\n","    def train(self, mode=True):\n","        r\"\"\"Sets the module in training mode.\"\"\"      \n","        self.training = mode\n","        for module in self.classification_model.children():\n","            module.train(mode)\n","        return self\n","\n","    def eval(self):\n","        r\"\"\"Sets the module in evaluation mode.\"\"\"\n","        return self.classification_model.train(False)\n","\n","model = Model(segmentation_model,classification_model)\n","\n","loss = build_loss({\"name\": \"CrossEntropyLoss\",\n","                   \"reduction\": \"sum\"})\n","\n","#optimizer = SGD(momentum=0.9, weight_decay=1e-4, nesterov=True)\n","optimizer = Adam(weight_decay=1e-4)\n","\n","suffix = datetime.now().isoformat()\n","base_folder = f\"{os.path.dirname(os.path.abspath('__file__'))}/output_{suffix}\"\n","try:\n","    os.makedirs(base_folder)\n","except OSError:\n","  pass\n","logger.info(f\"Logging outputs to {base_folder}\")\n","\n","num_epoch = 400\n","\n","if valid:\n","  task = ClassificationTask() \\\n","          .set_model(model) \\\n","          .set_dataset(dataset_train_classy, \"train\") \\\n","          .set_dataset(dataset_test_classy, \"test\") \\\n","          .set_loss(loss) \\\n","          .set_optimizer(optimizer) \\\n","          .set_optimizer_schedulers({\"lr\": LinearParamScheduler(start_value=0.01, end_value=0.0005)}) \\\n","          .set_num_epochs(num_epoch) \\\n","          .set_meters([AccuracyMeter([1,2])]) \\\n","          .set_dataloader_mp_context('fork') \\\n","          .set_hooks([ProgressBarHook(),LossLrMeterLoggingHook(),TensorboardPlotHook(SummaryWriter(log_dir=Path(base_folder) / \"tensorboard\"))])\n","\n","else:\n","  task = ClassificationTask() \\\n","          .set_model(model) \\\n","          .set_dataset(dataset_train_classy, \"train\") \\\n","          .set_loss(loss) \\\n","          .set_optimizer(optimizer) \\\n","          .set_optimizer_schedulers({\"lr\": LinearParamScheduler(start_value=0.01, end_value=0.0005)}) \\\n","          .set_num_epochs(num_epoch) \\\n","          .set_meters([AccuracyMeter([1,2])]) \\\n","          .set_dataloader_mp_context('fork') \\\n","          .set_hooks([ProgressBarHook(),LossLrMeterLoggingHook(),TensorboardPlotHook(SummaryWriter(log_dir=Path(base_folder) / \"tensorboard\"))])\n","\n","trainer = LocalTrainer()\n","trainer.train(task)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fTWsgSCNtaA"},"source":["save_type = 1\n","\n","model.eval()\n","\n","if save_type==0:\n","  with torch.no_grad():\n","    script = torch.jit.trace(model, torch.randn(1, 3, 240, 240, dtype=torch.float).cuda())\n","\n","    if valid:\n","      #torch.save(model.state_dict(), f'lemon_efficientnet_b1_valid_{now}_{num_epoch}.pt')\n","      torch.jit.save(script, f'lemon_efficientnet_b1_valid_{now}_{num_epoch}.pt')\n","      copyfile(f'lemon_efficientnet_b1_valid_{now}_{num_epoch}.pt',f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/lemon_efficientnet_b1_valid_{now}_{num_epoch}.pt')\n","    else:\n","      #torch.save(model.state_dict(), f'lemon_efficientnet_b1_{now}_{num_epoch}.pt')\n","      torch.jit.save(script, f'lemon_efficientnet_b1_{now}_{num_epoch}.pt')\n","      copyfile(f'lemon_efficientnet_b1_{now}_{num_epoch}.pt',f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/lemon_efficientnet_b1_{now}_{num_epoch}.pt')\n","elif save_type==1:\n","  torch.save(model.state_dict(), f'lemon_efficientnet_b1_{now}_{num_epoch}_type1.pt')\n","  copyfile(f'lemon_efficientnet_b1_{now}_{num_epoch}_type1.pt',f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/lemon_efficientnet_b1_{now}_{num_epoch}_type1.pt')\n","elif save_type==2:\n","  torch.save(model, f'lemon_efficientnet_b1_{now}_{num_epoch}_type2.pt')\n","  copyfile(f'lemon_efficientnet_b1_{now}_{num_epoch}_type2.pt',f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/lemon_efficientnet_b1_{now}_{num_epoch}_type2.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhavQWY8qfvF"},"source":["from torch import no_grad\n","\n","total_test = len(dataset_test)\n","\n","if valid:\n","\n","  from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n","\n","  result = [[0 for _ in range(4)] for _ in range(4)]\n","\n","  model.eval()\n","  with no_grad():\n","      for i in range(total_test):\n","        test = dataset_test_classy[i]\n","        predict = model(test['input'][None, :, :, :].cuda())\n","        predict_softmax = torch.log_softmax(predict, dim = 1)\n","        _, predict_class = torch.max(predict_softmax, dim = 1)\n","        result[test['target']][predict_class.item()] += 1\n","\n","  result = np.array(result)\n","  result = result.astype('float') / result.sum(axis=1)[:, np.newaxis]\n","\n","  fig, ax = plt.subplots(1,1, figsize=(10, 10))\n","  ConfusionMatrixDisplay(confusion_matrix=result, display_labels=[i for i in range(4)]).plot(include_values=True, ax=ax)\n","  fig.savefig('confusion.png')\n","  copyfile('confusion.png',f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/confusion_lemon_efficientnet_b1_{now}_{num_epoch}.png',)\n","\n","else:\n","\n","  import pandas as pd\n","\n","  result = []\n","\n","  model.eval()\n","  with no_grad():\n","      for i in range(total_test):\n","        test = image_transform_test(dataset_test[i])\n","        predict = model(test[None, :, :, :].cuda())\n","        predict_softmax = torch.log_softmax(predict, dim = 1)\n","        _, predict_class = torch.max(predict_softmax, dim = 1)\n","        result.append([f'test_{i:04}.jpg',predict_class.item()])\n","\n","  df = pd.DataFrame(result,columns=['id','class_num'])\n","  df.to_csv(f'lemon_efficientnet_b1_{now}_{num_epoch}.csv', index=False)\n","  copyfile(f'lemon_efficientnet_b1_{now}_{num_epoch}.csv',f'drive/MyDrive/Colab Notebooks/SIGNATEHiroshimaLemon/models/lemon_efficientnet_b1_{now}_{num_epoch}.csv')"],"execution_count":null,"outputs":[]}]}